{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "344faf1e",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8fb9dfb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted to sys.path: /home/luccasabbatini/github/uff/2025/genai_para_es/ai-requirements-priorization\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Native\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "# Third Party\n",
    "import torch\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from huggingface_hub import login\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Add project root to sys.path\n",
    "p = Path.cwd()\n",
    "for d in [p] + list(p.parents):\n",
    "    if (d / \"shared\").exists():\n",
    "        sys.path.insert(0, str(d))\n",
    "        print(\"Inserted to sys.path:\", d)\n",
    "        break\n",
    "else:\n",
    "    print(\"Warning: 'shared' folder not found in parent dirs\")\n",
    "\n",
    "# Local\n",
    "from shared.prompts import CALCULATE_PRIORITIZATION_PROMPT\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2bf2df",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b32a67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "HUGGINGFACE_HUB_TOKEN = os.getenv(\"HUGGINGFACE_HUB_TOKEN\")\n",
    "MODEL_NAME = \"Qwen/Qwen2.5-7B-Instruct\"\n",
    "MAX_TOKENS = 1000\n",
    "DATASET = \"ralic\"\n",
    "SOFTWARE_PROJECT = DATASET.split(\"/\")[-1]\n",
    "DATASET_PATH = f\"../data/{DATASET}/\"\n",
    "OUTPUT_PATH = f\"../results/{DATASET}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68567f7c",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e6f7f0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging (safe for notebook re-runs)\n",
    "root_logger = logging.getLogger()\n",
    "\n",
    "if not root_logger.handlers:\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\"\n",
    "    )\n",
    "else:\n",
    "    root_logger.setLevel(logging.INFO)\n",
    "    for h in root_logger.handlers:\n",
    "        h.setLevel(logging.INFO)\n",
    "    root_logger.propagate = False\n",
    "    \n",
    "# Login to Hugging Face Hub\n",
    "login(token=HUGGINGFACE_HUB_TOKEN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "800e09f1",
   "metadata": {},
   "source": [
    "### Verify GPU Availability and Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4794a851",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 21:24:31,471 - INFO - Torch CUDA version: 12.4; GPU: NVIDIA GeForce RTX 4060 Ti\n"
     ]
    }
   ],
   "source": [
    "# Log GPU info\n",
    "if torch.cuda.is_available():\n",
    "    logging.info(\n",
    "        f\"Torch CUDA version: {torch.version.cuda}; GPU: {torch.cuda.get_device_name(0)}\"\n",
    "    )\n",
    "else:\n",
    "    logging.info(\"No GPU found, training on CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171eb32a",
   "metadata": {},
   "source": [
    "### Setup Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9410f54e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 21:24:36,380 - INFO - Based on the current allocation process, no modules could be assigned to the following devices due to insufficient memory:\n",
      "  - 0: 2179989504 bytes required\n",
      "These minimum requirements are specific to this allocation attempt and may vary. Consider increasing the available memory for these devices to at least the specified minimum, or adjusting the model config.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 37.61it/s]\n",
      "2025-12-01 21:24:36,837 - WARNING - Some parameters are on the meta device because they were offloaded to the cpu and disk.\n",
      "2025-12-01 21:24:36,839 - INFO - Model device: cpu\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# Check if model is using GPU or CPU\n",
    "logging.info(f\"Model device: {next(model.parameters()).device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2911c2",
   "metadata": {},
   "source": [
    "### Calculate Cost, Risk, and Value for Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb8a0c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-01 22:00:12,082 - INFO - Processing requirement 1 of 50\n"
     ]
    }
   ],
   "source": [
    "# Load requirements csv into pandas DataFrame\n",
    "requirements_df = pd.read_csv(os.path.join(DATASET_PATH, f\"{SOFTWARE_PROJECT}.csv\"))\n",
    "software_description_file = os.path.join(DATASET_PATH, f\"{SOFTWARE_PROJECT}.md\")\n",
    "\n",
    "with open(software_description_file, \"r\", encoding=\"utf-8\") as f:\n",
    "\t\tsoftware_description = f.read()\n",
    "\n",
    "# Calculate Cost, Risk, and Value for each requirement\n",
    "calculation_results = [\"ID, Requirement, Description, Cost, Risk, Value\"]\n",
    "\n",
    "for index, row in requirements_df.iterrows():\n",
    "    logging.info(f\"Processing requirement {index + 1} of {len(requirements_df)}\")\n",
    "\n",
    "    requirement_id = row['ID']\n",
    "    requirement_text = row['Requirement']\n",
    "    requirement_description = row['Description']\n",
    "\n",
    "    prompt = CALCULATE_PRIORITIZATION_PROMPT.format(\n",
    "\t\t\t\tsoftware_description=software_description,\n",
    "\t\t)\n",
    "\n",
    "    # Prepare input for model\n",
    "    messages = [\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\t\"role\": \"system\",\n",
    "\t\t\t\t\t\t\"content\": prompt,\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\t\"role\": \"user\",\n",
    "\t\t\t\t\t\t\"content\": f\"ID,Requirement,Description\\n\\\"{requirement_id}\\\",\\\"{requirement_text}\\\",\\\"{requirement_description}\\\"\",\n",
    "\t\t\t\t},\n",
    "\t\t]\n",
    "\n",
    "    # Generate response from model\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=MAX_TOKENS)\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "    # Store the result\n",
    "    calculation_results.append(response.strip())\n",
    "\n",
    "    logging.info(f\"Completed requirement {index + 1}\")\n",
    "\n",
    "# Save results to CSV file\n",
    "output_file = os.path.join(OUTPUT_PATH, f\"{SOFTWARE_PROJECT} extended.csv\")\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "\t\tf.write(\"\\n\".join(calculation_results))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138ebeb6",
   "metadata": {},
   "source": [
    "### Clean Up GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d466f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "\t\ttorch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
